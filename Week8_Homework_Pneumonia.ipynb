{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week8_Homework_Pneumonia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoloso/hello-world/blob/master/Week8_Homework_Pneumonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lye5s9CPypf"
      },
      "source": [
        "# Week 8 Homework: Pneumonia Classification on X-Ray Images\n",
        "\n",
        "An application of Computer Vision for healthcare is training models on X-ray images to detect Pneumonia. In this homework we will use CNNs and transfer learning to detect pnemonia in Kaggle's Chest X-Ray Images dataset.\n",
        "\n",
        "For this homework to run, you must download the data zip file yourself as such:\n",
        "- Go to [this Kaggle link](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) and download the dataset (you may need to create a Kaggle account)\n",
        "- Rename the zip file to \"chest_xray.zip\" and upload it to your Google Drive at the root Folder\n",
        "\n",
        "Before you run the below cell, be sure to make sure that you are using GPU (Edit->Notebook settings) because we will be training larger networks than before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO9X465UXEOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f99c82-deb1-43bd-aae0-122c846819e3"
      },
      "source": [
        "import requests, zipfile, io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import GlobalAveragePooling2D, Dense\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhgz_glVSEEZ"
      },
      "source": [
        "!unzip /content/drive/MyDrive/chest_xray.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi1w0U2x2zsG"
      },
      "source": [
        "## Part 1: Visualize the Data\n",
        "\n",
        "Just as in previous assignments, we begin by looking to better understand our data. Here, your job is to use the function *visualize_train_image* below to display a finite number of images. You can find image file names in `filtered_train_data` above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f60RLO2l2e4-"
      },
      "source": [
        "def visualize_train_image(set_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(12, 6)\n",
        "\n",
        "    pneumonia_images = os.path.join('chest_xray', set_name, 'PNEUMONIA')\n",
        "    pneumonia_path = os.path.join(pneumonia_images, os.listdir(pneumonia_images)[0])\n",
        "      \n",
        "    im = Image.open(pneumonia_path)\n",
        "    ax1.imshow(im)\n",
        "\n",
        "    normal_images = os.path.join('chest_xray', set_name, 'NORMAL')\n",
        "    normal_path = os.path.join(normal_images, os.listdir(normal_images)[0])\n",
        "      \n",
        "    im = Image.open(normal_path)\n",
        "    ax2.imshow(im)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCE_vwmS2zIh"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "### END YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC1D0w356D6A"
      },
      "source": [
        "## Part 2: Exploring CNN Architectures \n",
        "\n",
        "At this point in the course, you've had the opportunity to construct and train your own convolutional neural networks with extra bells and whistles like pooling or batch normalization layers. Your CNNs were likely only a few layers deep, which constrains the representative power of the models. \n",
        "\n",
        "Thankfully, researchers around the world have been working on stacking more and more layers to create deeper architectures. However, early on, these researchers found that deeper networks actually performed worse. Why could this be?\n",
        "\n",
        "The authors of an architecture called [ResNet](https://arxiv.org/abs/1512.03385) observed something simple: direct mappings are hard to learn. Instead of trying to learn an underlying mapping from $x$ to $f(x)$, we can learn the difference between the two, or the “residual.” Then, to calculate $f(x)$, we can just add the residual to the input. Say the residual is $r(x) = f(x) - x$. Now, instead of trying to learn $f(x)$ directly, our networks are trying to learn $r(x)+x$. You can check out this [blog post](https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41) to learn more.\n",
        "\n",
        "We'll be exploring a 50-layer ResNet CNN architecture in Keras in Part 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiITuMs6BRpi"
      },
      "source": [
        "Before we jump into training our ResNet though, we need to load our data in with Keras data generators. Data generators are cool because they quickly iterate through your sets in batches. The batch size is a hyperparameter you can tune. We've put together that code for you. You can go ahead and play around with the batch size later when submitting your final model (note that batch sizes above 64 might cause memory errors, because very large batches will not fit on GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM91JFk5MXtt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "ea680576-c7de-4cab-b001-d16f3cbb2db9"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "batch_size = 32 \n",
        "### END YOUR CODE ###\n",
        "\n",
        "def get_data_generator(set_name, batch_size):\n",
        "    datagen = ImageDataGenerator(rescale = 1./255, horizontal_flip = set_name == 'train')\n",
        "    generator = datagen.flow_from_directory('./chest_xray/' + set_name,\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'categorical')\n",
        "    return generator\n",
        "    \n",
        "train_generator = get_data_generator('train', batch_size)\n",
        "val_generator = get_data_generator('val', batch_size)\n",
        "test_generator = get_data_generator('test', batch_size)\n",
        "\n",
        "num_classes = 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0bb677a1efb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0bb677a1efb8>\u001b[0m in \u001b[0;36mget_data_generator\u001b[0;34m(set_name, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m                                             \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                             class_mode = 'categorical')\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './chest_xray/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzEhoBN5Cp76"
      },
      "source": [
        "We can see that there are 2 classes.\n",
        "\n",
        "Keras has an easy API for loading the ResNet-50. You can play around with the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdyMHPVPCWGt"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "lr = 1e-3\n",
        "### END YOUR CODE ###\n",
        "\n",
        "def get_base_resnet_model(lr):\n",
        "    model = keras.applications.ResNet50(include_top = True, weights = None, \n",
        "                                        classes = num_classes, input_shape=(224, 224, 3), \n",
        "                                        input_tensor = None)\n",
        "\n",
        "    optim = optimizers.Adam(lr = lr)\n",
        "    model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "base_resnet_model = get_base_resnet_model(lr)\n",
        "base_resnet_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-2CsufTDQoa"
      },
      "source": [
        "Now that we have our 50-layer architecture set up, let's train it for 5 epochs! Refer to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator) to fit the model to the data from your generators. This will take around 10 minutes (make sure to run on GPU)! The training script will print out loss and accuracies as it trains–note that accuracies will be lower than you've seen before because we have many more output classes and we are only training for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkmyJjdpDXWx"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_4OvTb-ELoi"
      },
      "source": [
        "## Part 3: Transfer Learning\n",
        "\n",
        "Good job training your first super-deep network! For some historical context, ResNet was created for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). From the ImageNet [website](http://image-net.org/challenges/LSVRC/), \"ILSVRC evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress... across a wider variety of objects -- taking advantage of the quite expensive labeling effort.\" In other words, ImageNet is a huge labeled dataset that researchers can use to benchmark their architectures. \n",
        "\n",
        "After ResNet-50 is trained on ImageNet, we can save the weights and use them to bootstrap another task with the same architecture. This technique is broadly called **transfer learning**. As discussed in this [blog post](https://machinelearningmastery.com/transfer-learning-for-deep-learning/), \"In transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.\" In this case, the task is image classification on a different dataset.\n",
        "\n",
        "In Part 4, you'll have the opportunity to use a ResNet initialized with **pretrained weights** and see how it does on your task. Note that the ImageNet dataset has 1000 classes but our dataset only has 31 classes, so we'll have to replace the last layer of our ResNet to reflect that difference. Again, you can play around with the learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA2_LfJiUoBm"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "lr = 1e-3\n",
        "### END YOUR CODE ###\n",
        "\n",
        "def get_pretrained_resnet_model(lr):\n",
        "    # here, we pass in weights = 'imagenet' instead of weights = None\n",
        "    pretrained_resnet = keras.applications.ResNet50(include_top = False, weights = 'imagenet', \n",
        "                                                    classes = num_classes, input_shape = (224, 224, 3), \n",
        "                                                    input_tensor = None)\n",
        "    # replace last layer (including the pooling)\n",
        "    h = GlobalAveragePooling2D()(pretrained_resnet.output)\n",
        "    y_hat = Dense(num_classes, activation = 'softmax', name = 'fc1000')(h)\n",
        "    \n",
        "    model = Model(inputs = pretrained_resnet.input, outputs = y_hat)\n",
        "\n",
        "    optim = optimizers.Adam(lr = lr)\n",
        "    model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "pretrained_resnet_model = get_pretrained_resnet_model(lr)\n",
        "pretrained_resnet_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIGrGnOSHyEp"
      },
      "source": [
        "Train your new pretrained model for 1 epoch. Refer to the code for *base_resnet_model* above to see how to use *fit_generator*!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQczxGLjHxf9"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd3g94-_ImMB"
      },
      "source": [
        "## Part 4: Transfer Learning with Frozen Layers\n",
        "\n",
        "As you might have noticed, training these 50-layer ResNets end-to-end takes quite a bit more time than our previous smaller models. One of the big benefits of transfer learning is that it can oftentimes speed up training. \n",
        "\n",
        "In CNNs, features are more generic in early layers and more original-dataset-specific in later layers. This means that the initial layers of the ResNet model are still useful for this new task. So, we can choose to keep our ImageNet weights in earlier layers and only finetune them in later layers. We do this by freezing earlier layers and making them untrainable. We thus introduce an additional hyperparameter: the number of frozen layers. Note that due to different definitions for what a \"layer\" means, model.layers contains 177 layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjKT7k9HIly1"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "lr = 1e-3\n",
        "num_freeze_layers = 30\n",
        "### END YOUR CODE ###\n",
        "\n",
        "def get_pretrained_frozen_resnet_model(lr, num_freeze_layers):\n",
        "    pretrained_resnet = keras.applications.ResNet50(include_top = False, weights = 'imagenet', \n",
        "                                                    classes = 31, input_shape = (224, 224, 3), \n",
        "                                                    input_tensor = None)\n",
        "    # replace last layer (including the pooling)\n",
        "    h = GlobalAveragePooling2D()(pretrained_resnet.output)\n",
        "    y_hat = Dense(num_classes, activation = 'softmax', name = 'fc1000')(h)\n",
        "    \n",
        "    model = Model(inputs = pretrained_resnet.input, outputs = y_hat)\n",
        "\n",
        "    optim = optimizers.Adam(lr = lr)\n",
        "    model.compile(optimizer = optim, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    print('Freezing %d of %d model layers...' % (num_freeze_layers, len(model.layers)))\n",
        "    \n",
        "    if not num_freeze_layers:\n",
        "        for layer in model.layers[:num_freeze_layers]:\n",
        "            layer.trainable = False\n",
        "        for layer in model.layers[num_freeze_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "    return model\n",
        "    \n",
        "pretrained_frozen_resnet_model = get_pretrained_frozen_resnet_model(lr, num_freeze_layers)\n",
        "pretrained_frozen_resnet_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNf7loq8KehW"
      },
      "source": [
        "Train your new pretrained model with frozen layers for 5 epoch. Notice that training is a bit faster now, depending on how many layers you are freezing, since we are training fewer layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIKPYkCXKeI0"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3BppKwsKoxG"
      },
      "source": [
        "## Part 5: Evaluation\n",
        "\n",
        "At this point, we've trained a base 50-layer ResNet and a pretrained ResNet initialized with ImageNets weights. We've also finetuned a pretrained ResNet in Part 5. After completing hyperparameter tuning above (and also training for more epochs, if you like), pick the best of your three models according to validation performance and run evaluation on your test set. Note that you do not have to wait for training to complete to get an idea of what hyperparameters are doing well–you can keep track of how quickly the loss decreases.\n",
        "\n",
        "Remember: to avoid bias, only run evaluation on the test set once! Use validation accuracy (reported at the end of an epoch) to tune hyperparameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwzyWb9QHZWR"
      },
      "source": [
        "### BEGIN YOUR CODE ###\n",
        "# Save your best model to best_model here\n",
        "\n",
        "loss, acc = best_model.evaluate_generator(test_generator, len(test_generator))\n",
        "\n",
        "print('Your best test loss was', loss)\n",
        "print('Your best test accuracy was', acc)\n",
        "### END YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr2uDScXNJSU"
      },
      "source": [
        "## And that's a wrap!\n",
        "\n",
        "The skills covered in this notebook are very critical for deep learning in practice, because machine learning engineers don't want to keep reinventing the wheel. Good work finishing up this assignment! "
      ]
    }
  ]
}